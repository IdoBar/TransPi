# TransPi - TRanscriptome ANalysiS PIpeline

# General info
TransPi is TRanscriptome ANalysiS PIpeline based on the scientific workflow manager [Nextflow](https://www.nextflow.io). It is designed to help researchers get the best reference transcriptome assembly for their organisms of interest by performing multiple assemblies with different parameters to then get the best non-redundant consensus assembly. It also performs other valuable analyses such as quality assessment of the assembly, BUSCO scores, Transdecoder (ORFs), and gene ontologies (Trinotate). All these with minimum input from the user but without losing the potential of an comprehensive analysis.

# Detailed description
TransPi uses a vast amount of programs to generate the best consensus assembly. For a detailed view of the steps see **Figure 1**. After the precheck and reads info is set, reads enter the pipeline with a normalization process. Then multiple assemblies are going to be made using the info provided by the user (i.e. kmers). All these assemblies are concatenated together and use as input for the creation of a non-redundant reference assembly generated using EvidentialGene (more info here = **LINK**). This reference assembly is used for all downstream analyses routinely made to transcriptomes. Quality assessment, BUSCO, gene ontologies, and some basic stats are outputted by TransPi. This can be used by the user to assess the assembly and compared to other common methods for the creation of transcriptomes (e.g. Trinity only assembly).

** FIGURE 1 FLOWCHART HERE**

# Why use TransPi?
Even though most transcriptome analyses are only based on Trinity, this does not mean is the "*one fits all*" solution for transcriptomes assembly. Gene sizes, heterozygosity, read length, kmers, quality, and many others affect the performance of the assemblers. Our method is designed to avoid such problems by using multiple assemblers and different parameters for the assemblies. All combined transcripts are reduced to a reference transcriptome without the redundancy created by over assembly of the reads (i.e. multiple programs and parameters).

Example: Long genes are not always assembled correctly by Trinity. However, Velvet (with long kmers) is more suitable for such genes, thus compensating the missing genes in the transcriptome generated by Trinity.

We have tested the pipeline with various non-model organisms that often present difficulty in these analyses. **More info here**  

**INFO here about the test we made and the improvement**

# What TransPi uses for the analyses?  

List of programs use by TransPi:
- IDBA  
- SOAP  
- Trinity  
- Velvet  
- EvidentialGene  
- CD-Hit  
- BUSCO  
- Trandecoder  
- Trinotate  
- Diamond  
- SQLite  
- Hmmer  
- Exonerate  
- Blast  
- Bowtie2  
- R  
- Python  
- Java  

Databases used by TransPi:
- Swissprot
- Uniprot custom database (e.g. all metazoan proteins)
- Pfam

Installing all these programs can be confusing and time consuming. Thus, apart from the automatization for the transcriptome analysis by TransPi, we also developed a script to install all dependencies and databases needed for the pipeline. Also, it will create all the necessary config files needed to run the analysis. More info on the "How to use TransPi" section below.  


# How to use TransPi

## REQUIREMENTS   

Current version of TransPi only has two major requirements:  
- Linux machine
- Directory with the reads to analyze  

Directories **before** the precheck is run  

  reads = with paired-end reads (e.g. IndA_R1.fastq.gz, IndA_R2.fastq.gz).    
  		**Make sure reads end with _R1.fastq.gz and _R2.fastq.gz**  

    **Multiple individuals can be added to the same directory.**  
    
 
## DOWNLOADING TransPi   

1- Clone the repository   

```

git clone https://github.com/rivera10/TransPi.git  

```  

2- Move to the TransPi directory  

```

cd TransPi

```  


## INSTALLATION AND CONFIGURATION  

1- After the requirements are satisfied you can run the precheck script. The precheck run needs a PATH as an argument for running and installing (locally) all the databases and programs the pipeline needs.   

```

bash precheck_TransPi.sh /home/ubuntu/TransPi

```  

2- If the precheck run was successful all the programs and databases for running TransPi were installed (locally) in the specified PATH. Proceed to modify the file `nextflow.config` for the kmer list to be used for your reads and your reads length.  

Example for read lengths of 100bp:    

```  
    // kmers list (depends on read length!)
    k="25,37,43,55,63"

    //#maximal read length
    max_rd_len="100"

```

**NOTES**  
1- If you combined multiple libraries of the same individual to create a reference transcriptome, to later use in downstream analyses (e.g. Differential Expression), make sure the kmer list is based on the smaller reads length.  

Example: Combining reads of 100bp with 125bp  

2- There is other values you can modify as well such as insert size and minimum aligned length. If not sure leave the values as it is.  


## RUNNING TransPi   

To run the pipeline  
```

./nextflow run TransPi.nf --all -with-conda ~/anaconda3/envs/TransPi


```

**Mandatory arguments**  

- `--all`   
To run the full pipeline, starting with assemblies down to ontology analysis. Other options still in development  

- `-with-conda`  
Indicates the installation of TransPi to the pipeline  

</br>
</br>

## NOTES

- If an error occurs and you need to resume the pipeline just include the `-resume` option when calling the pipeline.  

```

./nextflow run TransPi.nf --all -with-conda ~/anaconda3/envs/TransPi -resume


```   

</br>  

- The `template.nextflow.config` file has different configurations for the each program of the pipeline (*e.g.* some with a lot of CPUs, others with a small amount of CPUs). You can modify this depending on the resources you have in your system. Example:

```

process {
    withLabel: big_cpus {
        cpus='30'
        memory='15 GB'
        clusterOptions='-p lemmium --qos=normal'
        executor='slurm'
    }


```  

In this case, the processes using the label `big_cpus` will use 30 CPUs. If your system only has 20 please modify this values accordingly to avoid errors. Also, you will notice that we are using [SLURM](https://slurm.schedmd.com/documentation.html) as our job manager in our server. If you do not need this specification just simply erase of the following lines that are custom for our system.    

```
	//erase this lines
        clusterOptions='-p lemmium --qos=normal'
        executor='slurm'
```  
In the contrary, if your system is using `SLURM`, `SGE`, `PBS/Torque`, etc., change the `executor` part for the one ofm your system.

The line `clusterOptions` can be used to add any other option that you will usually use for your job submission.  

Example of `SLURM`:

```

#SBATCH --get-user-env                                                                                                           
#SBATCH --clusters=inter                                                                                                  
#SBATCH --partition=teramem_inter                                                                                              
#SBATCH --mail-user=user@mail.com                                                                                            
#SBATCH --mail-type=ALL

```
Can be used like this:

```

clusterOptions='--get-user-env --clusters=inter --partition=teramem_inter --mail-user=user@mail.com --mail-type=ALL'


```

</br>

- The precheck run is designed to create a new `nextflow.config` every time is run with with the various `PATH` to the databases. You can modify the values that do not need editing for your analysis on the `template.nextflow.config` to avoid doing the changes after the precheck run.   

</br>

- To avoid calling the pipeline using `./nextflow` you can modify the nextflow command like this `chmod 777 nextflow`. For running the pipeline you just need to use:  

```

nextflow TransPi.nf

```    

- To monitor your pipeline remotely without connecting to the server via ssh use [Nextflow Tower](https://tower.nf/login). Make an account with your email and follow their instructions. After this, you can now run the pipeline adding the `-with-tower` option.

```

./nextflow run TransPi.nf --all -with-tower -with-conda ~/anaconda3/envs/TransPi  


```


## Future work
- Other options such as only perform assemblies (in progress)
- Docker and Singularity for running the pipeline (in progress)
- Create graphs of results 
