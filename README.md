# TransPi - Transcriptome analysis pipeline 

# General info
TransPi is TRanscriptome ANalysiS PIpeline based on the scientific workflow manager [Nextflow](https://www.nextflow.io). It is designed to help researchers get the best reference transcriptome assembly for their organisms of interest by performing multiple assemblies with different parameters to then get the best non-redundant consensus assembly. It also performs other valuable analyses such as quality assessment of the assembly, BUSCO scores, Transdecoder (ORFs), and gene ontologies (Trinotate). All these with minimum input from the user but without losing the potential of an comprehensive analysis.

# Detailed description
TransPi uses a vast amount of programs to generate the best consensus assembly. For a detailed view of the steps see **Figure 1**. After the precheck and reads info is set, reads enter the pipeline with a normalization process. Then multiple assemblies are going to be made using the info provided by the user (i.e. kmers). All these assemblies are concatenated together and use as input for the creation of a non-redundant reference assembly generated using EvidentialGene (more info here = **LINK**). This reference assembly is used for all downstream analyses routinely made to transcriptomes. Quality assessment, BUSCO, gene ontologies, and some basic stats are outputted by TransPi. This can be used by the user to assess the assembly and compared to other common methods for the creation of transcriptomes (e.g. Trinity only assembly).

** FIGURE 1 FLOWCHART HERE**

# Why use TransPi?
Even though most transcriptome analyses are only based on Trinity, this does not mean is the "*one fits all*" solution for transcriptomes assembly. Gene sizes, heterozygosity, read length, kmers, quality, and many others affect the performance of the assemblers. Our method is designed to avoid such problems by using multiple assemblers and different parameters for the assemblies. All combined transcripts are reduced to a reference transcriptome without the redundancy created by over assembly of the reads (i.e. multiple programs and parameters). 

Example: Long genes are not always assembled correctly by Trinity. However, Velvet (with long kmers) is more suitable for such genes, thus compensating the missing genes in the transcriptome generated by Trinity. 

We have tested the pipeline with various non-model organisms that often present difficulty in these analyses. **More info here**  

**INFO here about the test we made and the improvement**

# What TransPi uses for the analyses?  

List of programs use by TransPi:
- IDBA  
- SOAP  
- Trinity  
- Velvet  
- EvidentialGene  
- CD-Hit  
- BUSCO  
- Trandecoder  
- Trinotate  
- Diamond  
- SQLite  
- Hmmer  
- Exonerate  
- Blast  
- Bowtie2  
- R  
- Python  
- Java  

Databases used by TransPi:
- Swissprot
- Uniprot custom database (e.g. all metazoan proteins)
- Pfam

Installing all these programs can be confusing and time consuming. Thus, apart from the automatization for the transcriptome analysis by TransPi, we also developed a script to install all dependencies and databases needed for the pipeline. Also, it will create all the necessary config files needed to run the analysis. More info on the "How to use TransPi" section below.  


# How to use TransPi

## REQUIREMENTS   
- Directories **before** the precheck is run  

  reads = with paired-end reads (e.g. IndA_R1.fastq.gz, IndA_R2.fastq.gz).    
  		**Make sure reads end with _R1.fastq.gz and _R2.fastq.gz**  

    **Multiple individuals can be added to the same directory.**

## STEPS before running the pipeline    
1- After the requirements are satisfied you can now run the precheck. The precheck run needs a PATH as an argument for running and installing (locally) all the databases and programs the pipeline needs.   

```

bash precheck_TransPi.sh /home/ubuntu/TransPi

```  

2- If the precheck run was successful, a file called `nextflow.config` will be created. Modify in the `nextflow.config` the kmer list for your reads and read lengths. For instance if your reads length are 100bp a possibles kmers list could be "25,37,43,55,63".  

Example for read lengths of 75bp:    

```  
    // kmers list (depends on read length!)
    k="25,37,45,53"

    //#maximal read length
    max_rd_len="76"

```

If you combined multiple libraries of the same individual to create a reference transcriptome to later use in downstream analyses (e.g. Differential Expression) make sure the kmer list is based on the smaller reads lengths.  


3- If your installation of conda is not located on `~/anaconda3` modify the `nextflow.config` for the `PATH` of your conda installation. Example:  

```

    //Examples: ~/anaconda3...    ~/tools/anaconda3...   ~/tools/py3/anaconda3...
    condash="~/anaconda3"

```  

## RUNNING the pipeline (VM testing)    

1- Run the pipeline.   

```

./nextflow TransPi_short.nf

```

2- If an error occur and you need to resume the run just include the `-resume` when calling the pipeline.  

```

./nextflow TransPi_short.nf -resume

```    


## NOTES
1- The precheck run is designed to create a new `nextflow.config` every time is run with with the respectives `PATH` to the databases. You can modify the values that do not need editing for your analysis on the `template.nextflow.config` to avoid doing the changes after the precheck run.  

2- The `template.nextflow.config` file has different configuration for the each program of the pipeline (*e.g.* some with a lot of CPUs, others with a small amount of CPUs). You can modify this depending on the resources you have in your system. Example:

```

process {
    withLabel: big_cpus {
        cpus='30'
        memory='15 GB'
        clusterOptions='-p lemmium --qos=normal'
        executor='slurm'
    }


```

In this case, the processes using the label `big_cpus` will use 30 CPUs. If your system only has 20 please modify this values accordingly to avoid errors. Also, you will notice that we are using `SLURM` as our job manager in our server. If you do not need this specification just simply erase of the following lines that are custom for our system.    

```
	//erase this lines
        clusterOptions='-p lemmium --qos=normal'
        executor='slurm'
```  


3- To avoid calling the pipeline using `./nextflow` you can modify the nextflow command like this `chmod 777 nextflow`. For running the pipeline you just need to use:  

```

nextflow TransPi_short.nf

```    


## OPTIONAL   
Make an account with your email at [Nextflow Tower](https://tower.nf/login) for monitoring the pipeline on any internet browser without connecting to the server.  

Then run the pipeline adding the `-with-tower` option.

```

nextflow TransPi_short.nf -with-tower

```
